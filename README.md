# Must-read papers on Natural Language Processing (NLP)

<br>

# General
- [Beyond Accuracy: Behavioral Testing of NLP Models with CheckList](https://www.aclweb.org/anthology/2020.acl-main.442.pdf)



# [Transformers](https://huggingface.co/transformers/index.html#contents)
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
- [GPT: Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)
- [GPT-2: Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
- [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/abs/1901.02860)
- [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/abs/1906.08237)
- [XLM: Cross-lingual Language Model Pretraining](https://arxiv.org/abs/1901.07291)
- [RoBERTa: Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692)
- [DistilBERT: DistilBERT, a distilled version of BERT: smaller, faster, cheaper, and lighter](https://arxiv.org/abs/1910.01108)
- [CTRL: A Conditional Transformer Language Model for Controllable Generation](https://www.github.com/salesforce/ctrl)
- [CamemBERT: a Tasty French Language Model](https://arxiv.org/abs/1911.03894)
- [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942)
- [T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683)
- [XLM-RoBERTa: Unsupervised Cross-lingual Representation Learning at Scale](https://arxiv.org/abs/1911.02116)
- [MMBT: Supervised Multimodal Bitransformers for Classifying Images and Text](https://arxiv.org/pdf/1909.02950.pdf)
- [FlauBERT: Unsupervised Language Model Pre-training for French](https://arxiv.org/abs/1912.05372)
- [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/pdf/1910.13461.pdf)
- [ELECTRA: Pre-training text encoders as discriminators rather than generators](https://arxiv.org/abs/2003.10555)
- [DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation](https://arxiv.org/abs/1911.00536)
- [Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451)
- [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150)
- [GPT3: Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)
- [Big Bird: Transformers for Longer Sequences](https://arxiv.org/abs/2007.14062)

# GAN and VAE
- [SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient](https://arxiv.org/abs/1609.05473)
- [Generating Sentences from a Continuous Space](https://arxiv.org/abs/1511.06349)
- [Toward Controlled Generation of Text](https://arxiv.org/abs/1703.00955)
- [Adversarially Regularized Autoencoders](https://arxiv.org/abs/1706.04223)

# Dialouge Systems and Chatbots
- [A Neural Conversational Model](https://arxiv.org/abs/1506.05869)
- [A Knowledge-Grounded Neural Conversation Model](https://arxiv.org/abs/1702.01932)
- [Neural Approaches to Conversational AI](https://arxiv.org/abs/1809.08267)
- [Wizard of Wikipedia: Knowledge-Powered Conversational agents](https://arxiv.org/abs/1811.01241)
- [The Dialogue Dodecathlon: Open-Domain Knowledge and Image Grounded Conversational Agents](https://arxiv.org/abs/1911.03768)

# Q/A (Question Answering)
- [SQuAD: 100,000+ Questions for Machine Comprehension of Text](https://arxiv.org/abs/1606.05250)
- [Reading Wikipedia to Answer Open-Domain Questions](https://arxiv.org/abs/1704.00051)

# NER (Name Entity Recognition)
- [A Survey on Recent Advances in Named Entity Recognition from Deep Learning models](https://www.aclweb.org/anthology/C18-1182/)
- [A Survey on Deep Learning for Named Entity Recognition](https://arxiv.org/abs/1812.09449)
- [Named Entity Recognition With Parallel Recurrent Neural Networks](https://www.aclweb.org/anthology/P18-2012/)
- [Evaluating the Utility of Hand-crafted Features in Sequence Labelling](https://www.aclweb.org/anthology/D18-1310/)
- [Fast and Accurate Entity Recognition with Iterated Dilated Convolutions](https://arxiv.org/abs/1702.02098)
- [Neural Adaptation Layers for Cross-domain Named Entity Recognition](https://www.aclweb.org/anthology/D18-1226/)
- [Neural Architectures for Named Entity Recognition](https://www.aclweb.org/anthology/N16-1030/)

# Sentiment Analysis
- [Sentiment analysis using deep learning approaches: an overview](https://link.springer.com/article/10.1007/s11432-018-9941-6?error=cookies_not_supported&code=3c358a62-56b7-49a0-bbc8-e203d7244b53)

- [Sentiment analysis using deep learning architectures: a review](https://link.springer.com/article/10.1007/s10462-019-09794-5?error=cookies_not_supported&code=e3476e76-fa89-4b61-b071-339cde10f9fe)



# Data Augmentation
- add paper name and link

# Text Summarization
- add paper name and link

# Reading Comprehension
- add paper name and link

# NLI (Natural Language Inference)
- add paper name and link

# Image Captioning
- add paper name and link

# Emotion Recognition
- add paper name and link

# Dependency Parsing
- add paper name and link

# Semantic Parsing
- add paper name and link

# POS Tagging (Part-Of-Speech Tagging)
- add paper name and link

# Topic Modeling
- add paper name and link

# SRL (Semantic Role Labeling)
- add paper name and link

# Keyword Extraction
- add paper name and link

# Semantic Similarity
- add paper name and link

# Text Clustering
- add paper name and link

<br>

# Thanks to
- [Danial Alihosseini](https://github.com/Danial-Alh)
- [Mohammad Mahdi Samiei](https://github.com/mmsamiei)
- [MohammadMahdi Aghajani](https://github.com/mmaghajani)
