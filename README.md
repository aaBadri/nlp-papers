# Must-read papers on Natural Language Processing (NLP)

<br>

# General
- [Beyond Accuracy: Behavioral Testing of NLP Models with CheckList](https://www.aclweb.org/anthology/2020.acl-main.442)
- [Experience Grounds Language](https://www.aclweb.org/anthology/2020.emnlp-main.703)

# [Transformers](https://huggingface.co/transformers/index.html#contents)
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
- [GPT: Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)
- [What Does BERT Look At? An Analysis of BERT's Attention](https://arxiv.org/abs/1906.04341)
- [GPT-2: Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
- [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/abs/1901.02860)
- [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/abs/1906.08237)
- [XLM: Cross-lingual Language Model Pretraining](https://arxiv.org/abs/1901.07291)
- [RoBERTa: Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692)
- [DistilBERT: DistilBERT, a distilled version of BERT: smaller, faster, cheaper, and lighter](https://arxiv.org/abs/1910.01108)
- [CTRL: A Conditional Transformer Language Model for Controllable Generation](https://www.github.com/salesforce/ctrl)
- [CamemBERT: a Tasty French Language Model](https://arxiv.org/abs/1911.03894)
- [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942)
- [T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683)
- [XLM-RoBERTa: Unsupervised Cross-lingual Representation Learning at Scale](https://arxiv.org/abs/1911.02116)
- [MMBT: Supervised Multimodal Bitransformers for Classifying Images and Text](https://arxiv.org/pdf/1909.02950.pdf)
- [FlauBERT: Unsupervised Language Model Pre-training for French](https://arxiv.org/abs/1912.05372)
- [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/pdf/1910.13461.pdf)
- [ELECTRA: Pre-training text encoders as discriminators rather than generators](https://arxiv.org/abs/2003.10555)
- [DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation](https://arxiv.org/abs/1911.00536)
- [Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451)
- [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150)
- [GPT-3: Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)
- [Big Bird: Transformers for Longer Sequences](https://arxiv.org/abs/2007.14062)
- [MARGE: Pre-training via Paraphrasing](https://arxiv.org/abs/2006.15020)
- [Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention](https://arxiv.org/abs/2006.16236)
- [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://openreview.net/forum?id=YicbFdNTTy)
- [A Survey on Contextual Embeddings](https://arxiv.org/pdf/2003.07278.pdf)
- [Efficient Transformers: A Survey](https://arxiv.org/pdf/2009.06732.pdf)
- [How does Punctuation Affect Neural Models in Natural Language Inference (BERT, HBMP and BiLSTM)](https://aclanthology.org/2020.pam-1.15.pdf)
- [OPT: Open Pre-trained Transformer Language Models](https://arxiv.org/pdf/2205.01068.pdf)

# Multi Modal

# Language Models

## GAN
- [SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient](https://arxiv.org/abs/1609.05473)
- [Maximum-Likelihood Augmented Discrete Generative Adversarial Networks](https://arxiv.org/abs/1702.07983)
- [Adversarial Ranking for Language Generation](https://arxiv.org/abs/1705.11001)
- [Adversarial Feature Matching for Text Generation](https://arxiv.org/abs/1706.03850)
- [MaskGAN: Better Text Generation via Filling in the______](https://arxiv.org/abs/1801.07736)
- [Long Text Generation via Adversarial Training with Leaked Information](https://arxiv.org/abs/1709.08624)

## VAE
- [Generating Sentences from a Continuous Space](https://arxiv.org/abs/1511.06349)
- [Improved Variational Autoencoders for Text Modeling using Dilated Convolutions](http://proceedings.mlr.press/v70/yang17d.html)
- [Variational Lossy Autoencoder](https://openreview.net/forum?id=BysvGP5ee)
- [Lagging Inference Networks and Posterior Collapse in Variational Autoencoders](https://openreview.net/forum?id=rylDfnCqF7)
- [A Surprisingly Effective Fix for Deep Latent Variable Modeling of Text](https://doi.org/10.18653/v1/D19-1370)
- [Spherical Latent Spaces for Stable Variational Autoencoders](https://www.aclweb.org/anthology/D18-1480/)
- [Adversarially Regularized Autoencoders](https://arxiv.org/abs/1706.04223)
- [Implicit Deep Latent Variable Models for Text Generation](https://doi.org/10.18653/v1/D19-1407)

## Miscellaneous
- [Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks](https://arxiv.org/abs/1506.03099)
- [Professor Forcing: A New Algorithm for Training Recurrent Networks](https://arxiv.org/abs/1610.09038)
- [Toward Controlled Generation of Text](https://arxiv.org/abs/1703.00955)
- [Wasserstein Auto-Encoders](https://openreview.net/forum?id=HkL7n1-0b)
- [InfoVAE: Balancing Learning and Inference in Variational Autoencoders](https://doi.org/10.1609/aaai.v33i01.33015885)

# Dialouge Systems and Chatbots
- [A Neural Conversational Model](https://arxiv.org/abs/1506.05869)
- [A Knowledge-Grounded Neural Conversation Model](https://arxiv.org/abs/1702.01932)
- [Neural Approaches to Conversational AI](https://arxiv.org/abs/1809.08267)
- [Wizard of Wikipedia: Knowledge-Powered Conversational agents](https://arxiv.org/abs/1811.01241)
- [The Dialogue Dodecathlon: Open-Domain Knowledge and Image Grounded Conversational Agents](https://arxiv.org/abs/1911.03768)

# Q/A (Question Answering)
- [SQuAD: 100,000+ Questions for Machine Comprehension of Text](https://arxiv.org/abs/1606.05250)
- [Reading Wikipedia to Answer Open-Domain Questions](https://arxiv.org/abs/1704.00051)

# NER (Name Entity Recognition)
- [A Survey on Recent Advances in Named Entity Recognition from Deep Learning models](https://www.aclweb.org/anthology/C18-1182/)
- [A Survey on Deep Learning for Named Entity Recognition](https://arxiv.org/abs/1812.09449)
- [Named Entity Recognition With Parallel Recurrent Neural Networks](https://www.aclweb.org/anthology/P18-2012/)
- [Evaluating the Utility of Hand-crafted Features in Sequence Labelling](https://www.aclweb.org/anthology/D18-1310/)
- [Fast and Accurate Entity Recognition with Iterated Dilated Convolutions](https://arxiv.org/abs/1702.02098)
- [Neural Adaptation Layers for Cross-domain Named Entity Recognition](https://www.aclweb.org/anthology/D18-1226/)
- [Neural Architectures for Named Entity Recognition](https://www.aclweb.org/anthology/N16-1030/)

# Machine Translation
- [Word Translation Without Parallel Data](https://arxiv.org/abs/1710.04087)
- [Unsupervised Machine Translation Using Monolingual Corpora Only](https://arxiv.org/abs/1711.00043)

# Text Classification

## Using Graph
- [Semi-Supervised Classification with Graph Convolutional Networks](https://arxiv.org/abs/1609.02907)
- [Graph Convolutional Networks for Text Classification](https://arxiv.org/abs/1809.05679)

## Semi-supervised
- [Adversarial Training Methods for Semi-Supervised Text Classification](https://arxiv.org/abs/1605.07725)
- [Unsupervised Data Augmentation for Consistency Training](https://arxiv.org/abs/1904.12848)
- [Uncertainty-aware Self-training for Text Classification with Few Labels](https://arxiv.org/abs/2006.15315)

## Sentiment Analysis
- add paper name and link

# Data Augmentation
- [Back-translation paper: Improving Neural Machine Translation Models with Monolingual Data](https://arxiv.org/abs/1511.06709)
- [Understanding Back-Translation at Scale](https://arxiv.org/abs/1808.09381)
- [Contextual Augmentation: Data Augmentation by Words with Paradigmatic Relations](https://www.aclweb.org/anthology/N18-2072/)
- [EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks](https://arxiv.org/abs/1901.11196)

# Information Retrieval
- [Pretrained Transformers for Text Ranking: BERT and Beyond](https://arxiv.org/pdf/2010.06467.pdf)

# Common Sense Reasoning
- [Large Language Models are Zero-Shot Reasoners](https://arxiv.org/pdf/2205.11916.pdf)
- [Generated Knowledge Prompting for Commonsense Reasoning](https://arxiv.org/pdf/2110.08387.pdf)

# Text Summarization
- add paper name and link

# Reading Comprehension
- add paper name and link

# NLI (Natural Language Inference)
- add paper name and link

# Image Captioning
- add paper name and link

# Emotion Recognition
- add paper name and link

# Dependency Parsing
- add paper name and link

# Semantic Parsing
- add paper name and link

# POS Tagging (Part-Of-Speech Tagging)
- add paper name and link

# Topic Modeling
- add paper name and link

# SRL (Semantic Role Labeling)
- add paper name and link

# Keyword Extraction
- add paper name and link

# Semantic Similarity
- add paper name and link

# Text Clustering
- add paper name and link

<br>

# Thanks to
- [Danial Alihosseini](https://github.com/Danial-Alh)
- [Mohammad Mahdi Samiei](https://github.com/mmsamiei)
- [MohammadMahdi Aghajani](https://github.com/mmaghajani)
- [Farane Jalali Farahani](https://github.com/farJfar)
- [Mohammad Mahdi Abdollahpour](https://github.com/mahdiabdollahpour)
- [Soroush Rezaei](https://github.com/thinkingparticle)
- [Ali Karimi](https://github.com/AliKarimi74)
- [Keivan Ipchi Hagh](https://github.com/keivanipchihagh)
